{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "-JgWbAhaB5af"
   },
   "source": [
    "In this assignment you will learn how to apply the REINFORCE algorithm within the OpenAI Gym environment. Make sure OpenAI gym is installed on your machine. Now let's import some relevant packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M9ICC_WuB5ai"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers, logger\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from chainer import Chain\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "from chainer.optimizers import Adam\n",
    "from chainer import Variable\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xRXnDbwuB5ap"
   },
   "source": [
    "We will make use of the classic CartPole environment provided by OpenAI Gym. Figure out what the details of this environment are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rwzPfMhMB5as"
   },
   "outputs": [],
   "source": [
    "env_id = 'CartPole-v0'\n",
    "\n",
    "# You can set the level to logger.DEBUG or logger.WARN if you want to change the amount of output.\n",
    "logger.set_level(logger.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Making new env: CartPole-v0\n",
      "\n",
      ">> episode 0\n",
      "observation [ 0.02180306  0.04762818 -0.00456282 -0.02453863] action 0 reward 1.0 info {}\n",
      "observation [ 0.02275563 -0.14742804 -0.00505359  0.26670119] action 1 reward 1.0 info {}\n",
      "observation [ 0.01980707  0.04776567  0.00028043 -0.02757139] action 1 reward 1.0 info {}\n",
      "observation [ 2.07623790e-02  2.42883596e-01 -2.70994966e-04 -3.20165823e-01] action 0 reward 1.0 info {}\n",
      "observation [ 0.02562005  0.04776551 -0.00667431 -0.02756837] action 1 reward 1.0 info {}\n",
      "observation [ 0.02657536  0.24298253 -0.00722568 -0.32234962] action 1 reward 1.0 info {}\n",
      "observation [ 0.03143501  0.43820663 -0.01367267 -0.61730245] action 1 reward 1.0 info {}\n",
      "observation [ 0.04019914  0.63351689 -0.02601872 -0.9142601 ] action 1 reward 1.0 info {}\n",
      "observation [ 0.05286948  0.82898092 -0.04430392 -1.21500555] action 0 reward 1.0 info {}\n",
      "observation [ 0.0694491   0.63445765 -0.06860403 -0.93652806] action 1 reward 1.0 info {}\n",
      "observation [ 0.08213825  0.83043436 -0.08733459 -1.24995538] action 0 reward 1.0 info {}\n",
      "observation [ 0.09874694  0.63653354 -0.1123337  -0.9858564 ] action 1 reward 1.0 info {}\n",
      "observation [ 0.11147761  0.8329661  -0.13205083 -1.31160483] action 0 reward 1.0 info {}\n",
      "observation [ 0.12813693  0.63974014 -0.15828293 -1.06299945] action 0 reward 1.0 info {}\n",
      "observation [ 0.14093174  0.44702705 -0.17954292 -0.82388447] action 1 reward 1.0 info {}\n",
      "observation [ 0.14987228  0.64409082 -0.1960206  -1.16723103] action 0 reward 1.0 info {}\n",
      "\n",
      "Episode finished after 16 timesteps\n",
      "\n",
      ">> episode 1\n",
      "observation [ 0.03780968  0.0034462   0.02575267 -0.00633069] action 1 reward 1.0 info {}\n",
      "observation [ 0.0378786   0.19818954  0.02562606 -0.29077839] action 1 reward 1.0 info {}\n",
      "observation [ 0.04184239  0.39293689  0.01981049 -0.57527027] action 0 reward 1.0 info {}\n",
      "observation [ 0.04970113  0.19754291  0.00830509 -0.2764129 ] action 0 reward 1.0 info {}\n",
      "observation [0.05365199 0.00230346 0.00277683 0.01887784] action 0 reward 1.0 info {}\n",
      "observation [ 0.05369806 -0.1928582   0.00315439  0.31243559] action 0 reward 1.0 info {}\n",
      "observation [ 0.04984089 -0.38802495  0.0094031   0.60611164] action 1 reward 1.0 info {}\n",
      "observation [ 0.0420804  -0.19303574  0.02152533  0.31640522] action 0 reward 1.0 info {}\n",
      "observation [ 0.03821968 -0.38845757  0.02785344  0.61579802] action 1 reward 1.0 info {}\n",
      "observation [ 0.03045053 -0.19373562  0.0401694   0.33201624] action 0 reward 1.0 info {}\n",
      "observation [ 0.02657582 -0.38940563  0.04680972  0.63709112] action 0 reward 1.0 info {}\n",
      "observation [ 0.0187877  -0.58514803  0.05955154  0.94414014] action 1 reward 1.0 info {}\n",
      "observation [ 0.00708474 -0.39087671  0.07843435  0.67074753] action 0 reward 1.0 info {}\n",
      "observation [-7.32790506e-04 -5.86996428e-01  9.18492964e-02  9.87058742e-01] action 1 reward 1.0 info {}\n",
      "observation [-0.01247272 -0.39321643  0.11159047  0.72457972] action 0 reward 1.0 info {}\n",
      "observation [-0.02033705 -0.58969019  0.12608207  1.05019687] action 0 reward 1.0 info {}\n",
      "observation [-0.03213085 -0.78623852  0.147086    1.37964912] action 1 reward 1.0 info {}\n",
      "observation [-0.04785562 -0.59322689  0.17467899  1.13634531] action 1 reward 1.0 info {}\n",
      "observation [-0.05972016 -0.40076584  0.19740589  0.9031418 ] action 1 reward 1.0 info {}\n",
      "\n",
      "Episode finished after 19 timesteps\n"
     ]
    }
   ],
   "source": [
    "### TEST\n",
    "env = gym.make('CartPole-v0')\n",
    "for i_episode in range(2):\n",
    "    print('\\n>> episode', i_episode)\n",
    "    observation = env.reset()\n",
    "    for t in range(20):\n",
    "#         env.render()\n",
    "        print('observation',observation, end=' ')\n",
    "        action = env.action_space.sample()\n",
    "        print('action', action, end=' ')\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        print('reward', reward, 'info', info)\n",
    "        if done:\n",
    "            print(\"\\nEpisode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "789ayFxUB5a1"
   },
   "source": [
    "Let's define a baseline agent which just emits random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ymm5rSs-B5a3"
   },
   "outputs": [],
   "source": [
    "class RandomAgent(object):\n",
    "    \"\"\"The world's simplest agent!\"\"\"\n",
    "\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        return self.action_space.sample()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E9MqsR1LB5bC"
   },
   "source": [
    "Let's run the agent on the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1354
    },
    "colab_type": "code",
    "id": "CS2L_duwB5bE",
    "outputId": "40ccd223-7fe4-400f-8f7d-e38e39612c69"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n",
      " 30%|███       | 304/1000 [00:00<00:00, 3036.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Making new env: CartPole-v0\n",
      "<class 'gym.spaces.discrete.Discrete'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 3474.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 25.  39.  26.  14.  22.  27.  34.  33.  11.  13.  16.  36.  21.  16.\n",
      "  59.  12.  35.  15.  14.  38.   8.  24.  26.  12.  13.  40.  36.  10.\n",
      "  13.  19.  32.  40.  21.  12.  17.  58.  16.  13.  13.  15.  21.  29.\n",
      "  29.  27.  16.   9.  28.  27.  10.  19.  14.  27.  11.  16.  50.  62.\n",
      "  16.  24.  62.  13.  30.  17.  41.  55.  15.  13.  43.  20.  19.  19.\n",
      "  29.  19.  24.  10.  20.  23.  22.  17.  20.  18.  15.  13.   9.  10.\n",
      "  14.  15.  14.  16.  34.  12.  14.  14.  15.  14.  40.  41.  63.  16.\n",
      "  22.  11.  23.  21.  23.  11.  42.  30.  23.   9.  20.  16.  16.  24.\n",
      "  29.  10.  60.  28.  11.  14.  22.  21.  14.  21.  16.  15.  40.  14.\n",
      "  44.  42.  17.  12.  19.  36.  14. 119.  12.  24.  16.  16.  48.  11.\n",
      "  14.  13.  37.  17.  23.  22.  31.  13.  12.  21.  31.  30.  36.  28.\n",
      "  13.  12.  25.  52.  18.  35.  12.  20.  18.  21.  33.  10.  39.  24.\n",
      "  20.  33.  13.  30.  32.  15.  16.  30.  24.  15.  23.  19.  15.  15.\n",
      "   9.  82.  17.  21.  46.  27.  19.  18.  50.  17.  41.  17.  18.  17.\n",
      "  19.  13.  34.  35.  18.  14.  30.  30.  90.  27.  14.  15.  12.  35.\n",
      "  45.  16.  14.  15.  38.  16.  18.  18.  16.  13.  15.  10.  12.  16.\n",
      "  14.  37.  22.  41.  45.  21.  18.  18.  24.  29.  26.  30.  14.  29.\n",
      "  14.  13.  22.  24.  12.  23.  17.  12.  33.  17.  37.  17.  21.  40.\n",
      "  79.  18.  13.  26.  32.  13.  10.  18.  18.  10.  53.  13.  17.  21.\n",
      "  14.  12.  15.  58.  19.  18.  23.  22.  16.  17.  14.   9.  25.  16.\n",
      "  47.   9.  12.  29.  29.  43.  23.  39.  36.  28.  19.  14.  13.  11.\n",
      "  12.  34.  15.  24.  23.  14.  13.  27.  23.  21.  10.  46.  22.  11.\n",
      "  42.  15.  22.  11.  38.  22.  24.  17.  11.  42.  12.  26.  15.  28.\n",
      "  20.  36.  19.  29.  14.  33.  14.  14.  16.  18.  18.  10.  10.  23.\n",
      "  26.  22.  19.  13.  20.  20.  31.  36.  30.  18.  12.  20.  18.  25.\n",
      "  15.  18.  46.  20.   9.  22.  14.  12.  16.  24.  20.  11.  17.  22.\n",
      "  36.  35.  18.   9.  17.  13.  12.  34.  14.  11.  10.  13.  40.  17.\n",
      "  22.   8.  58.  23.  24.  24.  12.  35.  36.  15.  16.  37.  29.  15.\n",
      "  45.  33.  14.  20.  16.  14.  18.  23.  19.  18.   9.  13.  23.  16.\n",
      "   9.  19.  11.  31.  25.  21.  11.  30.   8.  21.  13.  29.  11.  11.\n",
      "  13.  17.  21.  18.  15.  13.  33.   9.  44.  16.  30.  25.  14.  12.\n",
      "  22.  10.  14.  65.  37. 100.  29.  20.  12.  11.  32.  16.  22.  17.\n",
      "  13.  20.  48.  15.  11.  28.  22.  22.  19.  16.  14.  23.  14.  35.\n",
      "  19.  24.   9.  15.  16.  19.  27.  19.  29.  21.  26.  16.  30.  57.\n",
      "  24.  80.  25.  21.  19.  16.  23.  17.  44.  22.  49.  24.  46.  16.\n",
      "  18.  32.  13.  12.  27.  45.  11.  13.  10.  14.  18.  38.  14.  21.\n",
      "  20.  12.  14.  14.  24.  22.  14.  29.  15.  35.  30.  26.   9.  19.\n",
      "  14.  45.  19.  17.  19.  25.  36.  14.  41.  20.  17.  41.  20.  21.\n",
      "  25.  13.  35.  17.  17.  18.  24.  12.  10.  44.  16.  21.  18.  10.\n",
      "  59.  43.  31.  14.  16.  27.  28.  21.  15.  36.  13.  18.  43.  11.\n",
      "  23.  14.   9.  15.  21.  14.  19.  13.  16.  14.  20.   9.  34.  21.\n",
      "  80.  37.  59.  15.  30.  16.  36.  14.  23.  31.  25.  20.  20.  18.\n",
      "  16.  10.  14.  22.  19.  10.  21.  14.  42.  16.  19.  21.  46.  12.\n",
      "  16.  18.  31.  26.  10.  20.  13.  18.  16.  13.  15.  15.  15.  15.\n",
      "  16.  16.  13.  30.  19.  47.  18.  27.  21.  14.  14.  11.  15.  16.\n",
      "  11.  14.  41.  11.  21.  17.  14.  17.  45.  12.  18.  23.  14.  19.\n",
      "  17.  19.  30.  23.  11.  23.  47.  35.  20.  24.  20.  20.  30.  29.\n",
      "  26.  39.  12.  10.  35.  22.  12.  10.  31.  30.  11.  20.  12.  37.\n",
      "  14.  26.  16.  13.  14.  19.  21.  13.  17.  17.  35.  11.  14.  17.\n",
      "  19.  13.   9.  14.  17.  41.  22.  17.  20.  29.  17.  16.  22.  12.\n",
      "  20.  37.  17.  14.  15.  36.  19.  14.  12.  36.  14.  19.  30.  10.\n",
      "  13.   9.  10.  44.  33.  24.  11.  18.  13.  16.  25.  21.  32.  15.\n",
      "  23.  30.  14.  20.  13.  13.  12.  15.  21.  31.  20.  51.  17.  30.\n",
      "  11.  10.  12.  34.  10.  22.  20.  33.  20.  11.   9.  14.  19.  17.\n",
      "  22.  13.  40.  35.  19.  21.  31.  12.  54.  24.  10.  16.  38.  14.\n",
      "  22.  20.  16.  12.  77.  15.  17.  14.   9.  30.  11.  16.  24.  15.\n",
      "  10.  24.  16.  13.  11.  30.  20.  29.  37.  14.  51.  28.  15.  17.\n",
      "  54.  16.   9.  18.  18.  25.  13.  28.  26.  15.  20.   9.  10.  15.\n",
      "  14.  11.  14.  29.  13.  12.  15.  18.  37.  32.  13.  57.  19.  47.\n",
      "  21.  10.  11.  16.  18.  76.  43.  16.  18.  27.  24.  17.  29.  28.\n",
      "  23.  17.  21.  37.  10.  22.  11.  32.  18.  18.  18.  32.  12.  17.\n",
      "  25.  22.  28.  23.  29.  23.  12.  24.  44.  21.  18.  17.  16.  20.\n",
      "  17.  48.  10.  16.  63.  14.  17.  12.  43.  31.  27.  12.  13.  23.\n",
      "  13.  22.  63.  12.  24.  20.  18.  15.  16.  23.  11.   9.  13.  13.\n",
      "  14.  23.  23.  20.  11.  16.  29.  53.  17.  14.  23.  11.  25.  30.\n",
      "  23.  25.  14.  11.  12.  23.  21.  19.  25.  15.  27.  46.  17.  17.\n",
      "  33.  12.  25.  13.  16.  17.  18.  10.  17.  31.  12.  13.  16.  18.\n",
      "  49.   9.  17.  15.  15.  21.  21.  55.  21.  34.  32.  11.  11.  11.\n",
      "  12.  13.  14.  15.   8.  21.  13.  18.  40.  30.  22.  11.  23.  12.\n",
      "  12.  16.  13.  28.  26.  13.  18.  17.  11.  11.  16.  43.  16.  33.\n",
      "  39.  16.  48.  24.  18.  18.  12.  19.  16.  14.  31.  28.  10.  10.\n",
      "  14.  12.  19.  21.  29.  11.]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_id)\n",
    "env.seed(0)\n",
    "print(type(env.action_space))\n",
    "agent = RandomAgent(env.action_space)\n",
    "\n",
    "episode_count = 1000\n",
    "done = False\n",
    "reward = 0\n",
    "    \n",
    "R0 = np.zeros(episode_count)\n",
    "for i in tqdm.trange(episode_count):\n",
    "\n",
    "    ob = env.reset()\n",
    "\n",
    "    while True:\n",
    "\n",
    "        action = agent.act(ob, reward, done)\n",
    "        ob, reward, done, _ = env.step(action)\n",
    "\n",
    "        R0[i] += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "# Close the env and write monitor result info to disk\n",
    "env.close()\n",
    "print(R0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9u6DXf-QB5bO"
   },
   "source": [
    "Let's create the REINFORCE agent. We assume that the policy is computed using an MLP with a softmax output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OV5EtXhzB5bR"
   },
   "outputs": [],
   "source": [
    "class MLP(Chain):\n",
    "    \"\"\"Multilayer perceptron\"\"\"\n",
    "\n",
    "    def __init__(self, n_output=1, n_hidden=5):\n",
    "        super(MLP, self).__init__(l1=L.Linear(None, n_hidden), \n",
    "                                  l2=L.Linear(n_hidden, n_output))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.l2(F.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g0GtdM-tB5bW"
   },
   "source": [
    "1: A skeleton for the REINFORCEAgent is given. Implement the compute_loss and compute_score functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JC6Tnd0PB5bZ"
   },
   "outputs": [],
   "source": [
    "class REINFORCEAgent(object):\n",
    "    \"\"\"Agent trained using REINFORCE\"\"\"\n",
    "\n",
    "    def __init__(self, action_space, model, optimizer=Adam()):\n",
    "\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.optimizer.setup(self.model)\n",
    "\n",
    "        # monitor score and reward\n",
    "        self.rewards = []\n",
    "        self.scores = []\n",
    "\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "\n",
    "        # linear outputs reflecting the log action probabilities and the value\n",
    "        policy = self.model(Variable(np.atleast_2d(np.asarray(observation, 'float32'))))\n",
    "\n",
    "        # generate action according to policy\n",
    "        p = F.softmax(policy).data\n",
    "\n",
    "        # normalize p in case tiny floating precision problems occur\n",
    "        row_sums = p.sum(axis=1)\n",
    "        p /= row_sums[:, np.newaxis]\n",
    "\n",
    "        action = np.asarray([np.random.choice(p.shape[1], None, True, p[0])])\n",
    "\n",
    "        return action, policy\n",
    "\n",
    "\n",
    "    def compute_loss(self):\n",
    "        \"\"\"\n",
    "        Return loss for this episode based on computed scores and accumulated rewards\n",
    "        \"\"\"\n",
    "    \n",
    "        return Variable(np.array([0]))\n",
    "\n",
    "    def compute_score(self, action, policy):\n",
    "        \"\"\"\n",
    "        Computes score\n",
    "\n",
    "        Args:\n",
    "            action (int):\n",
    "            policy:\n",
    "\n",
    "        Returns:\n",
    "            score\n",
    "        \"\"\"\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0LM2TB0B5bi"
   },
   "source": [
    "Now we run the REINFORCE agent on the CartPole environment. Note that we update the agent after each episode for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "bQEtBE4GB5bj",
    "outputId": "d4a99d27-c2b3-4a48-e21d-07ed965da535"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n",
      "  1%|          | 7/1000 [00:00<00:14, 67.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:12<00:00, 81.36it/s]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_id)\n",
    "env.seed(0)\n",
    "\n",
    "network = MLP(n_output=env.action_space.n, n_hidden=3)\n",
    "agent = REINFORCEAgent(env.action_space, network, optimizer=Adam())\n",
    "\n",
    "episode_count = 1000\n",
    "done = False\n",
    "reward = 0\n",
    "    \n",
    "R = np.zeros(episode_count)\n",
    "for i in tqdm.trange(episode_count):\n",
    "\n",
    "    ob = env.reset()\n",
    "\n",
    "    loss = 0\n",
    "    while True:\n",
    "\n",
    "        action, policy = agent.act(ob, reward, done)\n",
    "\n",
    "        ob, reward, done, _ = env.step(action[0])\n",
    "\n",
    "        # get reward associated with taking the previous action in the previous state\n",
    "        agent.rewards.append(reward)\n",
    "        R[i] += reward\n",
    "\n",
    "        # recompute score function: grad_theta log pi_theta (s_t, a_t) * v_t\n",
    "        agent.scores.append(agent.compute_score(action, policy))\n",
    "\n",
    "        # we learn at the end of each episode\n",
    "        if done:\n",
    "            \n",
    "            loss += agent.compute_loss()\n",
    "            \n",
    "            agent.model.cleargrads()\n",
    "            loss.backward()\n",
    "            loss.unchain_backward()\n",
    "            agent.optimizer.update()\n",
    "\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P4X55LLpB5br"
   },
   "outputs": [],
   "source": [
    "# You may want to run a video of the trained agent performing in the environment using the env.render() function.\n",
    "#\n",
    "# for i in range(3):\n",
    "#\n",
    "#     ob = env.reset()\n",
    "#\n",
    "#     while True:\n",
    "#\n",
    "#         action, policy = agent.act(ob, reward, done)\n",
    "#\n",
    "#         ob, reward, done, _ = env.step(action[0])\n",
    "#\n",
    "#         if done:\n",
    "#             break\n",
    "#       \n",
    "#         env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YOa8XPX1B5bu"
   },
   "source": [
    "2: Plot the cumulative reward for both RandomAgent and REINFORCEAgent."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of SOW-MKI49-2018-SEM1-V-assignment_6.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
